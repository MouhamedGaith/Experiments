# -*- coding: utf-8 -*-
"""Experiments.ipynb

Automatically generated by Colaboratory.

"""


# install StellarGraph if running on Google Colab
import sys
if 'google.colab' in sys.modules:
#   %pip install -q stellargraph[demos]==1.2.1

# verify that we're using the correct version of StellarGraph for this notebook
import stellargraph as sg

try:
    sg.utils.validate_notebook_version("1.2.1")
except AttributeError:
    raise ValueError(
        f"This notebook requires StellarGraph version 1.2.1, but a different version {sg.__version__} is installed.  Please see <https://github.com/stellargraph/stellargraph/issues/1172>."
    ) from None

from stellargraph import StellarGraph
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
import stellargraph as sg
import matplotlib.pyplot as plt

from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

import os
import networkx as nx
import numpy as np
import pandas as pd

from stellargraph.data import BiasedRandomWalk
from stellargraph import StellarGraph
from stellargraph import datasets
from IPython.display import display, HTML

# %matplotlib inline

VH = pd.read_csv(
    "/content/smart.link",
    sep="\t",  # tab-separated
    header=None,  # no heading row
    names=["target", "source"],  # set our own names for the columns
)
VH

VH_feature_names = [f"w{i}" for i in range(1433)]

VH_raw_content = pd.read_csv(
    "/content/smart.pattern",
    sep="\t",  # tab-separated
    header=None,  # no heading row
    names=["id", *VH_feature_names, "Pattern"],  # set our own names for the columns
)
VH_raw_content

VH_content_str_subject = VH_raw_content.set_index("id")
VH_content_str_subject

VH_content_no_subject = VH_content_str_subject.drop(columns="Pattern")
VH_content_no_subject

VH_no_subject = StellarGraph({"pattern": VH_content_no_subject}, {"symptom": VH_cites})
print(VH_no_subject.info())

VH_subject = VH_content_str_subject["Pattern"]
VH_subject

walk_length = 100  # maximum length of a random walk to use throughout this notebook

# specify the metapath schemas as a list of lists of node types.
metapaths = [
    ["symptom", "needs", "pattern"],
    ["symptom", "causes", "symptom", "symptom"],
    ["pattern", "pattern"],
]

from sklearn import model_selection

VH_train, VH_test = model_selection.train_test_split(
    VH_subject, train_size=0.25, random_state=123
)
VH_train

import networkx as nx
import pandas as pd
import os

import stellargraph as sg
from stellargraph.mapper import GraphSAGENodeGenerator
from stellargraph.layer import GraphSAGE

from tensorflow.keras import layers, optimizers, losses, metrics, Model
from sklearn import preprocessing, feature_extraction, model_selection
from stellargraph import datasets
from IPython.display import display, HTML
import matplotlib.pyplot as plt


target_encoding = preprocessing.LabelBinarizer()

train_targets = target_encoding.fit_transform(cora_train)
test_targets = target_encoding.transform(VH_test)

batch_size = 50
num_samples = [10, 5]

from stellargraph.data import BiasedRandomWalk

rw = BiasedRandomWalk(VH_no_subject)

walks = rw.run(
    nodes=list(cora_no_subject.nodes()), 
    length=100,  
    n=10,  
    p=0.5, 
    q=2.0,  
)
print("Number of random walks: {}".format(len(walks)))

generator = GraphSAGENodeGenerator(VH_no_subject, batch_size, num_samples)

train_gen = generator.flow(VH_train.index, train_targets, shuffle=True)

from gensim.models import Word2Vec

str_walks = [[str(n) for n in walk] for walk in walks]
model = Word2Vec(str_walks, size=128, window=5, min_count=0, sg=1, workers=2, iter=1)

model.wv["19231"].shape

graphsage_model = GraphSAGE(
    layer_sizes=[32, 32], generator=generator, bias=True, dropout=0.5,
)

x_inp, x_out = graphsage_model.in_out_tensors()
prediction = layers.Dense(units=train_targets.shape[1], activation="softmax")(x_out)

model = Model(inputs=x_inp, outputs=prediction)
model.compile(
    optimizer=optimizers.Adam(lr=0.005),
    loss=losses.categorical_crossentropy,
    metrics=["acc"],
)

test_gen = generator.flow(VH_test.index, test_targets)

history = model.fit(
    train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False
)


node_ids = model.wv.index2word  # list of node IDs
node_embeddings = (
    model.wv.vectors
) 
node_targets = node_subjects[[int(node_id) for node_id in node_ids]]

all_nodes = VH_subject.index
all_mapper = generator.flow(all_nodes)
all_predictions = model.predict(all_mapper)

# Apply t-SNE transformation on node embeddings
tsne = TSNE(n_components=2)
node_embeddings_2d = tsne.fit_transform(node_embeddings)

# draw the points
alpha = 0.7
label_map = {l: i for i, l in enumerate(np.unique(node_targets))}
node_colours = [label_map[target] for target in node_targets]

plt.figure(figsize=(10, 8))
plt.scatter(
    node_embeddings_2d[:, 0],
    node_embeddings_2d[:, 1],
    c=node_colours,
    cmap="jet",
    alpha=alpha,
)

# X will hold the 128-dimensional input features
X = node_embeddings
# y holds the corresponding target values
y = np.array(node_targets)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.1, test_size=None)
print(
    "Array shapes:\n X_train = {}\n y_train = {}\n X_test = {}\n y_test = {}".format(
        X_train.shape, y_train.shape, X_test.shape, y_test.shape
    )
)
clf = LogisticRegressionCV(
    Cs=10, cv=10, scoring="accuracy", verbose=False, multi_class="ovr", max_iter=300
)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(random_state=0)
rfc.fit(X_train, y_train)
y_pred = rfc.predict(X_test)
from sklearn.metrics import accuracy_score
print('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
svc=SVC() 
svc.fit(X_train,y_train)
y_pred=svc.predict(X_test)
print('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print('Confusion matrix\n\n', cm)

